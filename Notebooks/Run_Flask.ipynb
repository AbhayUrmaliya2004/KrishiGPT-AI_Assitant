{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import ngrok\n",
    "\n",
    "# Set the authtoken\n",
    "NGROK_TOKEN = \"Put your Ngrok Token Here\"\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "\n",
    "# Start the tunnel\n",
    "public_url = ngrok.connect(5000)\n",
    "print(f\"Public URL: {public_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and run the Flask File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# # Define the output directory for your model\n",
    "output_dir = \"/content/drive/MyDrive/LlamaResultsSaved\"  # Replace with your actual output directory\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True)\n",
    "\n",
    "# Configure quantization settings for 4-bit model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Load the base model with quantization\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Replace with your base model if necessary\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"  # Ensure you have this folder created\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "finetuned_model = PeftModel.from_pretrained(model, output_dir)\n",
    "\n",
    "# Initialize pipeline for text generation\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=finetuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=500,\n",
    "    device=0  # Ensure it uses GPU if available\n",
    ")\n",
    "\n",
    "# Flask route for text generation\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate_response():\n",
    "    \"\"\"API endpoint for generating responses.\"\"\"\n",
    "    data = request.get_json()\n",
    "    prompt = data.get('prompt', '')\n",
    "\n",
    "    if not prompt:\n",
    "        return jsonify({\"error\": \"Prompt is required\"}), 400\n",
    "\n",
    "    formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "    result = pipe(formatted_prompt)\n",
    "\n",
    "    response_text = result[0]['generated_text'][len(prompt) + 15:]\n",
    "    return jsonify({\"response\": response_text})\n",
    "\n",
    "# Run the Flask app\n",
    "def run_flask():\n",
    "    app.run(port=5000, use_reloader=False, debug=True)\n",
    "\n",
    "# Start Flask app in a separate thread\n",
    "flask_thread = threading.Thread(target=run_flask)\n",
    "flask_thread.start()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
